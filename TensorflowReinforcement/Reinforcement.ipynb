{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for model:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, nStates, nActions, nBatches):\n",
    "        \n",
    "        ### Misc variables ###\n",
    "        self._nStates = nStates\n",
    "        self._nBatches = nBatches\n",
    "        self._nActions = nActions\n",
    "        \n",
    "        ### Placeholders ###\n",
    "        self._states = None\n",
    "        self._actions = None\n",
    "        \n",
    "        ### outputs ###\n",
    "        self._logits = None\n",
    "        self._optimizer = None\n",
    "        self._var_init = None\n",
    "        \n",
    "        self._define_model()\n",
    "        \n",
    "    def _define_model(self):\n",
    "        self._states = tf.placeholder(shape=[None, self._nStates],\n",
    "                                      dtype=tf.float32)\n",
    "        self._qsa = tf.placeholder(shape=[None, self._nActions],\n",
    "                                   dtype = tf.float32)\n",
    "        \n",
    "        ### layers ###\n",
    "        x1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
    "        x2 = tf.layers.dense(x1, 50, activation=tf.nn.relu)\n",
    "        \n",
    "        ### outputs ###\n",
    "        self._logits = tf.layers.dense(x2, self._nActions)\n",
    "        \n",
    "        ### loss ###\n",
    "        loss = tf.losses.mean_squared_error(self._qsa, self._logits)\n",
    "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        self._var_init = tf.global_variables_initializer()\n",
    "        \n",
    "    def predict_one(self, state, sess):\n",
    "        dict2feed = {self._states : state.reshape(1, self._nStates)}\n",
    "        return sess.run(self._logits, feed_dict=dict2feed)\n",
    "    \n",
    "    def predict_batch(self, state, sess):\n",
    "        dict2feed = {self._states : state}\n",
    "        return sess.run(self._logits, feed_dict = dict2feed)\n",
    "    \n",
    "    def train_batch(self, sess, x_bach, y_batch):\n",
    "        dict2feed = {self._states: x_bach,\n",
    "                     self._qsa : y_batch}\n",
    "        sess.run(self._optimizer, feed_dict = dict2feed)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class to store (state, action) pairs\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxMemory):\n",
    "        self._samples = collections.deque(maxlen = maxMemory)\n",
    "        \n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        \n",
    "    # def sample(self, nSamples):\n",
    "    #     deqLen = len(self._samples)\n",
    "    #     if nSamples > deqLen: return np.random.sample(self._samples, deqLen)\n",
    "    #     else: return np.random.sample(self._samples, nSamples)\n",
    "        \n",
    "    def sample(self, nSamples):\n",
    "        deqLen = len(self._samples)\n",
    "        high = np.max([deqLen-1, 1])\n",
    "        \n",
    "        ### DEBUG ###\n",
    "        #print (\"dequelen: {}\".format(deqLen))\n",
    "        #print (\"nSamples: {}\".format(nSamples))\n",
    "        #print (\"High: {}\".format(high))\n",
    "        \n",
    "        size = nSamples if nSamples <= deqLen else deqLen\n",
    "        \n",
    "        randInts = np.random.randint(low = 0,\n",
    "                                     high = high,\n",
    "                                     size = size)\n",
    "        return [self._samples[i] for i in randInts]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GameRunner class:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameRunner:\n",
    "    def __init__(self, sess, model, env, memory,\n",
    "                 max_eps, min_eps, gamma, render=True):\n",
    "        self._sess = sess\n",
    "        self._env = env\n",
    "        self._model = model\n",
    "        self._memory = memory\n",
    "        self._render = render\n",
    "        self._max_eps = max_eps\n",
    "        self._min_eps = min_eps\n",
    "        self._decay = gamma\n",
    "        self._eps = self._max_eps\n",
    "        self._steps = 0\n",
    "        self._reward_store = []\n",
    "        self._max_x_store = []\n",
    "        \n",
    "        self._gamma = 0.5\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        state = self._env.reset()\n",
    "        tot_reward = 0\n",
    "        max_x = -100\n",
    "        \n",
    "        while True:\n",
    "            if self._render: self._env.render()    # If render is TRUE, render the environment to screen \n",
    "            action = self._choose_action(state)    # choose an action based on the state\n",
    "            next_state, reward, done, info = self._env.step(action)    # pick action, get reward and new state\n",
    "            \n",
    "            ### Custom increase to reward function ###\n",
    "            if next_state[0] >= 0.1: reward += 10\n",
    "            elif next_state[0] >= 0.25: reward += 20\n",
    "            elif next_state[0] >= 0.5: reward += 100\n",
    "            if next_state[0] > max_x: max_x = next_state[0]    # increase max_x if appropriate\n",
    "            \n",
    "            ### Is game complete? ###\n",
    "            if done: next_state = None\n",
    "            \n",
    "            ### Memory ###\n",
    "            self._memory.add_sample((state, action, reward, next_state))\n",
    "            self._replay()\n",
    "            \n",
    "            ## Exponentially decay the epsilon value\n",
    "            self._steps += 1\n",
    "            self._eps = self._min_eps + (self._max_eps - self._min_eps) \\\n",
    "                                  * math.exp(-self._decay * self._steps)\n",
    "            \n",
    "            ### move the agent to the next state and accumulate the reward ###\n",
    "            state = next_state\n",
    "            tot_reward += reward\n",
    "            \n",
    "            ### if game is done, break loop ###\n",
    "            if done:\n",
    "                self._reward_store.append(tot_reward)\n",
    "                self._max_x_store.append(max_x)\n",
    "                break\n",
    "        \n",
    "        print (\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
    "        \n",
    "        \n",
    "    def _choose_action(self, state):\n",
    "        if np.random.random() < self._eps: return np.random.randint(0, self._model._nActions - 1)\n",
    "        else: return np.argmax(self._model.predict_one(state, self._sess))\n",
    "        \n",
    "    def _replay(self):\n",
    "        batch = self._memory.sample(self._model._nBatches)\n",
    "        states = np.array([ x[0] for x in batch ])\n",
    "        next_states = np.array([ np.zeros(self._model._nStates) if x[3] is None else x[3] for x in batch])\n",
    "        \n",
    "        qsa = self._model.predict_batch(states, self._sess)\n",
    "        qsad = self._model.predict_batch(next_states, self._sess)\n",
    "        \n",
    "        x = np.zeros((len(batch), self._model._nStates))\n",
    "        y = np.zeros((len(batch), self._model._nActions))\n",
    "        \n",
    "        for i, b in enumerate(batch):\n",
    "            \n",
    "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
    "            \n",
    "            current_q = qsa[i]\n",
    "            \n",
    "            if next_state is None: current_q[action] = reward\n",
    "            else: current_q[action] = reward + self._gamma * np.amax(qsad[i])\n",
    "                \n",
    "            x[i] = state\n",
    "            y[i] = current_q\n",
    "            \n",
    "        self._model.train_batch(self._sess, x, y)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_EPSILON = 0.5\n",
    "MIN_EPSILON = 0.0\n",
    "GAMMA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nStates = env.env.observation_space.shape[0]\n",
    "nActions = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(nStates, nActions, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model._var_init)\n",
    "    gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON, GAMMA, render=False)\n",
    "    num_episodes = 300\n",
    "    count = 0\n",
    "    while count < num_episodes:\n",
    "        if count % 10 == 0:\n",
    "            print ('Episode {} of {}'.format(count+1, num_episodes))\n",
    "        gr.run()\n",
    "        count += 1\n",
    "    \n",
    "    plt.plot(gr.reward_store)\n",
    "    plt.show()\n",
    "    plt.close(\"all\")\n",
    "    plt.plot(gr.max_x_store)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
